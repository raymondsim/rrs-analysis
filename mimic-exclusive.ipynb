{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels and Report Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ROOT = \"/gpfs/users/a1808469\"\n",
    "\n",
    "DATASET = \"mimic-cxr\" \n",
    "\n",
    "RRS_FINDINGS_PATH = f\"{ROOT}/{DATASET}/text/train.findings.tok\"\n",
    "RRS_IMPRESSION_PATH = f\"{ROOT}/{DATASET}/text/train.impression.tok\"\n",
    "RRS_IMG_PATH = f\"{ROOT}/{DATASET}/text/train.image.tok\"\n",
    "\n",
    "IMG_LABEL_PATH = f\"{ROOT}/{DATASET}/{DATASET}-train-img-labels.json\"\n",
    "\n",
    "SENTENCE_MAPPING_PATH = f\"{ROOT}/{DATASET}/chexpert-labels/sentence_mappings_train_findings.json\"\n",
    "\n",
    "CHEXPERT_FINDINGS_LABELS_PATH = f\"{ROOT}/{DATASET}/chexpert-labels/train.findings.label\"\n",
    "CHEXPERT_IMPRESSION_LABELS_PATH = f\"{ROOT}/{DATASET}/chexpert-labels/train.impression.label\"\n",
    "\n",
    "# Get txt labels\n",
    "import csv\n",
    "with open(CHEXPERT_FINDINGS_LABELS_PATH, \"r\") as f:\n",
    "    reader = csv.DictReader(f)  # Reads CSV into a list of dictionaries\n",
    "    findings_labels = list(reader)\n",
    "\n",
    "# Convert txt labels to float \n",
    "findings_labels = [\n",
    "    {k: float(v) if k != \"Reports\" and v else v if k == \"Reports\" else np.nan for k, v in label.items()}\n",
    "    for label in findings_labels\n",
    "]\n",
    "\n",
    "with open(CHEXPERT_IMPRESSION_LABELS_PATH, \"r\") as f:\n",
    "    reader = csv.DictReader(f)  # Reads CSV into a list of dictionaries\n",
    "    impression_labels = list(reader)\n",
    "\n",
    "# Convert txt labels to float \n",
    "impression_labels = [\n",
    "    {k: float(v) if k != \"Reports\" and v else v if k == \"Reports\" else np.nan for k, v in label.items()}\n",
    "    for label in impression_labels\n",
    "]\n",
    "\n",
    "\n",
    "# Get sentence mappings\n",
    "import json\n",
    "with open(SENTENCE_MAPPING_PATH, \"r\") as f:\n",
    "    sentence_mappings = json.load(f)\n",
    "\n",
    "# Read original txt\n",
    "with open(RRS_FINDINGS_PATH, \"r\") as f:\n",
    "    input_text = [line.strip() for line in f]\n",
    "\n",
    "with open(RRS_IMPRESSION_PATH, \"r\") as f:\n",
    "    ground_truth = [line.strip() for line in f]\n",
    "\n",
    "with open(RRS_IMG_PATH, \"r\") as f:\n",
    "    image_paths = [line.strip() for line in f]\n",
    "\n",
    "# normalize each sample in original text (remove <q> and lower case)\n",
    "input_text = [s.replace(\"<q>\", \"\").lower() for s in input_text]\n",
    "\n",
    "# Get image labels\n",
    "with open(IMG_LABEL_PATH, \"r\") as f:\n",
    "    img_labels = json.load(f)\n",
    "\n",
    "# Convert img labels positive labels to match txt labels\n",
    "for d in img_labels:\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, (int, float)) and v > 0.5:\n",
    "            d[k] = 1.0\n",
    "        elif isinstance(v, (int, float)) and v < 0.1:\n",
    "            d[k] = -1.0\n",
    "        else:\n",
    "            d[k] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def exclusive(original_doc, mapped_sentence, disease, threshold=90):\n",
    "    new_doc = original_doc\n",
    "\n",
    "    for sent in sent_tokenize(original_doc):\n",
    "            if fuzz.ratio(sent.strip(), mapped_sentence[disease]) > threshold:\n",
    "                new_doc = new_doc.replace(sent, \" \")\n",
    "\n",
    "    return new_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modified_index = []\n",
    "exclusive_set = []\n",
    "exclusive_set_impression = []\n",
    "exclusive_set_images = []\n",
    "\n",
    "conflicting_set = []\n",
    "\n",
    "reduced_orginal_findings = []\n",
    "reduced_orginal_impression =[]\n",
    "reduced_orginal_images =[]\n",
    "\n",
    "block_count = 0\n",
    "exact_count = 0\n",
    "exact = []\n",
    "\n",
    "# Generate exclusive set\n",
    "for i, (input_txt, findings, img_label, impressions, sentences) in enumerate(zip(input_text, findings_labels, img_labels, impression_labels, sentence_mappings)):\n",
    "\n",
    "    exclusive_doc = input_txt # copy input text to temp, for some modifications\n",
    "    conflicting_doc = input_txt\n",
    "    blocked = False\n",
    "\n",
    "    # Normalize image labels to txt labels\n",
    "    if findings[\"No Finding\"] == 1.0 or impressions[\"No Finding\"] == 1.0:\n",
    "        # no_replacements.append(i)\n",
    "        continue\n",
    "\n",
    "    # Only continue to augmentation if contain other observation/disease\n",
    "\n",
    "    disease_list = [\n",
    "        \"Enlarged Cardiomediastinum\", \"Cardiomegaly\", \"Lung Opacity\", \"Edema\", \"Consolidation\",\n",
    "        \"Pneumonia\", \"Atelectasis\", \"Pneumothorax\", \"Pleural Effusion\", \"Pleural Other\"\n",
    "    ]\n",
    "\n",
    "    for disease in disease_list:\n",
    "        img_key = disease\n",
    "        if disease == \"Pleural Effusion\":\n",
    "            img_key = \"Effusion\"\n",
    "        elif disease == \"Pleural Other\":\n",
    "            img_key = \"Fibrosis\"\n",
    "\n",
    "        if findings[disease] == img_label.get(img_key, None) == impressions[disease]:\n",
    "            exclusive_doc = exclusive(input_txt, sentences, disease)\n",
    "            # conflicting_doc = conflicting(input_txt, sentences, disease)\n",
    "            blocked = True\n",
    "\n",
    "\n",
    "    # Special logic for Lung Lesion\n",
    "    if findings[\"Lung Lesion\"] == impressions[\"Lung Lesion\"]:\n",
    "        lesion_flag = any(img_label[k] == 1.0 for k in [\"Lung Lesion\", \"Mass\", \"Nodule\"]) or \\\n",
    "                      all(img_label[k] == 0.0 for k in [\"Lung Lesion\", \"Mass\", \"Nodule\"])\n",
    "\n",
    "        if lesion_flag:\n",
    "            exclusive_doc = exclusive(input_txt, sentences, \"Lung Lesion\")\n",
    "            blocked = True\n",
    "    \n",
    "\n",
    "    # After filtering, store if exc & conf successfully obtained\n",
    "    if conflicting_doc != input_txt and exclusive_doc != input_txt:\n",
    "        # conflicting_set.append(conflicting_doc)\n",
    "        exclusive_set.append(exclusive_doc)\n",
    "        reduced_orginal_findings.append(input_txt)\n",
    "        modified_index.append(i)\n",
    "\n",
    "# Get image and impression\n",
    "for index in modified_index:\n",
    "    reduced_orginal_impression.append(ground_truth[index])\n",
    "    reduced_orginal_images.append(image_paths[index])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(exclusive_set) == len(conflicting_set) == len(reduced_orginal_findings) == len(reduced_orginal_images) == len(reduced_orginal_impression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(exclusive_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to txt file\n",
    "def list_to_txt(docs, path):\n",
    "    with open(path, 'w') as file:\n",
    "        for string in docs:\n",
    "            file.write(string + '\\n')\n",
    "    print(f\"===Saved to {path} ===\")\n",
    "\n",
    "list_to_txt(exclusive_set, f\"{ROOT}/dataset/{DATASET}/text/train.findings.tok.exclusive\")\n",
    "\n",
    "list_to_txt(reduced_orginal_findings, f\"{ROOT}/dataset/{DATASET}/text/train.findings.tok.reduced\")\n",
    "list_to_txt(reduced_orginal_impression, f\"{ROOT}/dataset/{DATASET}/text/train.impression.tok.reduced\")\n",
    "list_to_txt(reduced_orginal_images, f\"{ROOT}/dataset/{DATASET}/text/train.image.tok.reduced\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchxray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
